package {{package}}.kafka.consumer;

{{#lombok}}
import lombok.RequiredArgsConstructor;
import lombok.extern.log4j.Log4j2;
{{/lombok}}
import org.springframework.boot.context.event.ApplicationStartedEvent;
import org.springframework.context.event.EventListener;
import org.springframework.kafka.core.reactive.ReactiveKafkaConsumerTemplate;
import org.springframework.stereotype.Component;
import reactor.core.publisher.Flux;
import reactor.core.publisher.Mono;
import reactor.core.scheduler.Schedulers;

{{#lombok}}
@Log4j2
@RequiredArgsConstructor
{{/lombok}}
public class KafkaConsumer {
{{^lombok}}
    private static final org.apache.logging.log4j.Logger log = org.apache.logging.log4j.LogManager.getLogger(KafkaConsumer.class);
{{/lombok}}
    private final ReactiveKafkaConsumerTemplate<String, String> kafkaConsumer;
    //private final SomeUseCase useCase;
{{^lombok}}

    public KafkaConsumer(final ReactiveKafkaConsumerTemplate<String, String> kafkaConsumer/*, final SomeUseCase useCase*/) {
        this.kafkaConsumer = kafkaConsumer;
        // this.useCase = useCase;
    }
{{/lombok}}

    @EventListener(ApplicationStartedEvent.class)
    public Flux<Object> listenMessages() {
        return kafkaConsumer
                .receiveAutoAck()
                .publishOn(Schedulers.newBoundedElastic(Schedulers.DEFAULT_BOUNDED_ELASTIC_SIZE, Schedulers.DEFAULT_BOUNDED_ELASTIC_QUEUESIZE, "kafka"))
                .flatMap(record -> {
                    // map record and process
                    // return userCase.something(record.value())
                    log.info("Record received {}", record.value());
                    return Mono.empty();
                })
                .doOnError(error -> log.error("Error processing kafka record", error))
                .retry()
                .repeat();
    }
}
